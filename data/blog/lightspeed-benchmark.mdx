---
title: 'I benchmarked OpenShift LightSpeed AI Assistant and this is what I found'
date: '2025-04-25'
tags: ['openshift', 'AI', 'lightspeed', 'benchmark']
draft: false
summary: 'I am using OpenShift LightSpeed AI assistant to solve all the exercises of a troubleshooting tutorial I created last year, and the results are insane.'
---

# TL;DR

The **OpenShift LightSpeed** AI Assistant is ridiculously useful.
Even in its current tech preview phase, I’m already impressed.
OpenShift admins and users are going to love it: the ability to ask general OpenShift question, generate YAML resources and demand,
and more importantly, **troubleshooting issue based on provided context (YAML, Logs, Events) is invaluable**.
For quick tests or lab environments, you can get it up and running in
minutes by connecting it to OpenAI’s cloud models - gpt-4o mini works perfectly well.
For disconnected clusters or environments where data can’t leave the premises,
adding a GPU to your cluster and deploying a local LLM is the way to go, and it's easy.
A well-trained 8-billion parameter conversational model is more than
sufficient, and **Granite 3.2-8B-Instruct** is my top pick at the time of writing.

This benchmark will demonstrate the power of OpenShift LightSpeed, and demonstrate that a narrow LLM with 8 billions parameters is enough.

<TOCInline toc={props.toc.filter((heading) => heading.depth <= 3)} asDisclosure />

# Introduction

AI assistants are a powerful way to boost productivity.
Yes, their answers still need cross-checking at times—but the pace of improvement is astonishing, and the productivity gains are already real.

In this post, I’ll share the results of a short, **unbiased** benchmark study that highlights how effective
OpenShift LightSpeed (the OpenShift AI Assistant) can be in troubleshooting scenarios.

Why is this benchmark unbiased? Because I created a set of hands-on OpenShift troubleshooting exercises about a year ago,
as a training asset for OpenShift users and admins.
Now that LightSpeed is available, I’m using the exact same scenarios to evaluate its capabilities. No retrofitting or tailored prompts.

The benchmark compares results across three different LLM providers.
LightSpeed itself needs an LLM backend, either cloud-hosted or self-managed, but thanks to its
RAG (Retrieval-Augmented Generation) integration with OpenShift documentation, no model retraining or fine-tuning is needed.

Here are the models I used:

- **OpenAI gpt-4o-mini**: Fast, powerful, affordable, and a leading option in the market. While OpenAI hasn’t disclosed the exact size of the model, it’s estimated to use **~8 billion parameters**. It performed extremely well in this benchmark.

- **IBM Granite-3.2-8B-Instruct**: Fully open source and indemnified, two characteristics that appeal to organizations running LLMs in their own data centers. At **8 billion parameters**, this model is a solid peer to gpt-4o mini. In this benchmark, it was deployed on a GPU (Spoiler: it performs just as well.)

- **IBM Granite-3.2-2B-Instruct**: At **only 2 billion parameters**, this model enters CPU-hosted territory. In my CPU tests, it's slow, too slow for a great user experience. But I included it (running on GPU) just to test whether small LLMs can still deliver good results (Spoiler: not yet). In this benchmark, it was deployed on a GPU.

What this benchmark is not:

- Not a speed or concurrency test. Both Granite models ran on a single GPU and were fast enough for single-user usage.

- Not a full LLM showdown. The goal is to showcase LightSpeed’s value even on smaller models, not rank every model on the market.

- Not a comparison of LLM servers solutions. I used the open-source vLLM server, where Red Hat (via Neural Magic) is now the top contributor. Quantization support is still maturing, and other servers might be better for large quantized models.

- Not biased. All troubleshooting scenarios were written long before LightSpeed was released. Prompts were initially tested with gpt-4o mini, so it may have a slight edge—but identical prompts were used across all models.

- Not covering all of LightSpeed’s capabilities. This post focuses on troubleshooting. While LightSpeed also answers general OpenShift questions and generates kubernetes YAML resources effectively, that’s outside the scope here.

# Benchmark protocol

This benchmark is based on six troubleshooting exercises (or "tests") that I created over a year ago as part of
a hands-on workshop to help OpenShift users learn how to diagnose and resolve issues.
You can access the full workshop—including instructions, hints, and solutions—here: https://trbl-workshop.cszevaco.com/workshop.

With a good understanding of both how LightSpeed and OpenShift work, and as the original author of the workshop,
I crafted a consistent set of questions to ask the AI assistant.
The answers were assessed based on their accuracy and effectiveness in helping resolve the problem.

For this benchmark,

- I used OpenShift LightSpeed Operator in version 0.3.2, still in tech preview at time of writing. Running on OpenShift 4.18.5
- I used vLLM version: 0.8.4 with GPU support (event for Granite 3.2-2b model)
- The chat history was cleared between each test to avoid context issues.

The table below summarizes the six tests, the context provided
(such as Kubernetes resource YAML, pod logs, and resource events),
the prompts used, and the expected answers.
Note that LightSpeed needs some context (YAML, logs...) to provide troubleshooting guidances specific to the problem at hand.

<table class="table-auto">
{/* <table border="1" cellpadding="8" cellspacing="0"> */}
  <thead>
    <tr>
      <th>Test</th>
      <th>Problem statement</th>
      <th>Context used</th>
      <th>Prompt used</th>
      <th>Expected results</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td rowspan="2">[1](https://trbl-workshop.cszevaco.com/workshop/exercise_01)</td>
      <td rowspan="2">The deployment specifies an incorrect image name, causing the pod to fail with ImagePullBackOff</td>
      <td rowspan="2">Full Deployment YAML + full Pod YAML + Pod events</td>
      <td>Why is the pod not starting?</td>
      <td>The answer should highlight a possible typo in the image name, show the full image name, and mention other potential causes, while emphasizing the typo as a likely root cause.</td>
    </tr>
    <tr>
      <td>There is a typo in the image name, what is the best approach to fix it?</td>
      <td>The answer should recommend editing the deployment, not the pod, to demonstrate understanding of the deployment-pod relationship.</td>
    </tr>

    <tr>
      <td rowspan="2">[2](https://trbl-workshop.cszevaco.com/workshop/exercise_02)</td>
      <td rowspan="2">The container image has a typo in its entrypoint command (e.g., start vs. starttt), causing the pod to start and crash immediately.</td>
      <td rowspan="2">Full Deployment YAML + full Pod YAML + full Pod log + Pod events</td>
      <td>What's wrong?</td>
      <td>The answer should identify the typo in the container’s entrypoint, recommend checking the script or command, and suggest rebuilding the image or updating the entrypoint to fix it.</td>
    </tr>
    <tr>
      <td>I am not allowed to rebuild the container image. How can I fix the issue?</td>
      <td>The answer should explain how to override the container’s command in the deployment specification to bypass the incorrect entrypoint without modifying the image.</td>
    </tr>

        <tr>
      <td rowspan="5">[3](https://trbl-workshop.cszevaco.com/workshop/exercise_03)</td>
      <td rowspan="5">The pod fails to start due to tight CPU and memory quotas in the namespace. Limits and requests must be tuned, but eventually, the deployment must be scaled down to one replica to fit within the quotas.</td>
      <td>Full Deployment YAML</td>
      <td>What's wrong?</td>
      <td>The answer should identify resource quotas as the cause and suggest setting appropriate CPU and memory requests and limits for the containers.</td>
    </tr>
      <tr>
      <td rowspan="2">Adding the namespace resource quotas</td>
      <td>What value do you suggest I should use for the deployment limits and requests?</td>
      <td>The answer should suggest resource values that fit within the quotas and ideally account for two replicas, or at least highlight the need to consider replica count when sizing limits and requests.</td>
    </tr>
    <tr>
      <td>Since I am running two replicas in the deployment, what value do you suggest I should use?</td>
      <td>The answer should clearly recommend CPU limits of 15m, memory limits of 15Mi, CPU requests of 10m, and memory requests of 7.5Mi.</td>
    </tr>
    <tr>
      <td rowspan="2">Adding one failing Pod events + full Pod YAML + full Pod logs</td>
      <td>Why is this pod not starting?</td>
      <td>The answer should explain that the pod is failing due to out-of-memory (OOM) or resource exhaustion, suggest increasing the namespace resource quotas or optimizing container resource usage, and ideally recommend scaling down to one replica.</td>
    </tr>
      <tr>
      <td>I can't optimize the application resources, and I can't change the resource quotas of the namespace. What are my other options?</td>
      <td>The answer should suggest scaling down the deployment to one replica to fit within the existing resource quotas.</td>
    </tr>

        <tr>
      <td rowspan="1">[4](https://trbl-workshop.cszevaco.com/workshop/exercise_04)</td>
      <td rowspan="1">The route is unavailable due to two misconfigurations in the service: an incorrect pod selector and an invalid port mapping.</td>
      <td>Full Deployment YAML + full Service YAML + full Route YAML</td>
      <td>What is wrong with my route or service?</td>
      <td>The answer should identify both the incorrect pod selector and the invalid service port configuration as the causes of the route being inaccessible.</td>
    </tr>

        <tr>
      <td rowspan="2">[5](https://trbl-workshop.cszevaco.com/workshop/exercise_05)</td>
      <td rowspan="2">The pod is unschedulable because the deployment specifies a node selector label that does not exist on any node.</td>
      <td>Full Deployment YAML + full Pod YAML.</td>
      <td>Why is the pod not deploying?</td>
      <td>The answer should identify an issue with the node selector preventing pod scheduling and suggest reviewing or correcting the node selector configuration.</td>
    </tr>
    <tr>
      <td>Adding one Node YAML</td>
      <td>There is a policy that forces the use of node selector in the deployment definition. Based on the node definition attached, can you suggest some labels that are likely to be presents in all nodes, so that my pod can be scheduled everywhere?</td>
      <td>The answer should provide a corrected node selector, ideally with clear instructions and an example shown in YAML format.</td>
    </tr>

        <tr>
      <td rowspan="6">[6](https://trbl-workshop.cszevaco.com/workshop/exercise_06)</td>
      <td rowspan="6">A namespace is stuck in terminating state due to lingering finalizers that have not been cleared.</td>
      <td>Full Namespace YAML</td>
      <td>Why is this namespace stuck in terminating?</td>
      <td>The answer should mention that the namespace is stuck due to resources with lingering finalizers and should identify which resources are preventing termination.</td>
    </tr>
    <tr>
      <td rowspan="2">Adding the full PVC YAML</td>
      <td>Why is this PVC stuck in terminating state?</td>
      <td>The answer should identify that a finalizer is blocking the PVC deletion and suggest checking the PVC’s owner reference or related resources.</td>
    </tr>
    <tr>
      <td>Can you provide me with an openshift command line that identify any pod using this PVC?</td>
      <td>The answer should provide an oc command to identify the pod(s) using the PVC or attached to the underlying PV.</td>
    </tr>
    <tr>
      <td rowspan="2">Adding the full POD YAML</td>
      <td>Why is this pod stuck in terminating state?</td>
      <td>The answer should identify that a non-standard finalizer is blocking pod termination, and recommend removing the finalizer as part of the resolution.</td>
    </tr>
        <tr>
      <td>How can I remove the finalizer?</td>
      <td>The answer should provide an oc command to patch the resource and remove the finalizer.</td>
    </tr>
        <tr>
      <td>Downloading the secret definition, and adding it via the upload option.</td>
      <td>Why is this secret not deleted, and how can I delete it?</td>
      <td>The answer should provide an oc command to patch the Secret and remove its finalizer, allowing the resource to be deleted.</td>
    </tr>

  </tbody>
</table>

Out of the 6 tests mentioned above, **many of my colleague were particularly impressed with test 4**.
That's the one you would want to dive a bit more first if needed.

The gif below illustrate the use of OpenShift LightSpeed to try and solve test 1.

![ols-usage-gif](/static/images/lightspeed-benchmark/ols2.gif)

# Results

## Test 1

### Test 1 Summary problem statement

<table class="table-auto">
  <thead>
    <tr>
      <th>Problem statement</th>
      <th>Context used</th>
      <th>Prompt used</th>
      <th>Expected results</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td rowspan="2">The deployment specifies an incorrect image name, causing the pod to fail with ImagePullBackOff</td>
      <td rowspan="2">Full Deployment YAML + full Pod YAML + Pod events</td>
      <td>Why is the pod not starting?</td>
      <td>The answer should highlight a possible typo in the image name, show the full image name, and mention other potential causes, while emphasizing the typo as a likely root cause.</td>
    </tr>
    <tr>
      <td>There is a typo in the image name, what is the best approach to fix it?</td>
      <td>The answer should recommend editing the deployment, not the pod, to demonstrate understanding of the deployment-pod relationship.</td>
    </tr>
  </tbody>
</table>
### Test 1 Summary results
<table class="table-auto">
  <thead>
    <tr>
      <th>Prompt used</th>
      <th>Expected results</th>
      <th>GPT-4o mini</th>
      <th>Granite-3.2-8B-Instruct</th>
      <th>Granite-3.2-2B-Instruct</th>

    </tr>

  </thead>
  <tbody>
    <tr>
      <td>Why is the pod not starting?</td>
      <td>The answer should highlight a possible typo in the image name, show the full image name, and mention other potential causes, while emphasizing the typo as a likely root cause.</td>
      <td style={{ backgroundColor: 'lightgreen' }}>The answer provide a list of potential issues, including the actual existence of this specific "pacmad" image</td>
      <td style={{ backgroundColor: 'lightgreen' }}>The answer suggest, among other things, to verify that the image exists</td>
      <td style={{ backgroundColor: 'lightgreen' }}>The answer ask to check that the image is accessible, it doesn't directly suggest the image wouldn't exist</td>
    </tr>
        <tr>
      <td>There is a typo in the image name, what is the best approach to fix it?</td>
      <td>The answer should recommend editing the deployment, not the pod, to demonstrate understanding of the deployment-pod relationship.</td>
      <td style={{ backgroundColor: 'lightgreen' }}>The answer suggest to fix the deployment via a series of oc commands</td>
      <td style={{ backgroundColor: 'lightgreen' }}>The answer suggest to edit the yaml file (via the oc edit command). The very impressive bit is that it says: Assuming the correct image name is quay.io/.../pacma**n**:latest Yes, it infered the correct name, very impressive.</td>
      <td style={{ backgroundColor: 'lightgreen' }}>The answer suggest to edit the yaml file, and provide an example assuming a fictitious image name</td>
    </tr>
  </tbody>
</table>

### Test 1 conclusion

gpt-4o-mini and granite3.2-8b and granite-3.2-2b all gave correct answers.

**granite-3.2-8b** was particularly impressive as **it managed to infer the likely correct image name**. Bonus point.

### Test 1 Detailed results

#### Prompt 1

<details>
  <summary>GPT-4o mini results</summary>

![test1-prompt1-gpt4o-mini](/static/images/lightspeed-benchmark/test1-prompt1-gpt4o-mini.webp)

</details>

<details>
  <summary>Granite-3.2-8B-Instruct results</summary>

![test1-prompt1-grantite-3.2-8b](/static/images/lightspeed-benchmark/test1-prompt1-granite-3.2-8b.webp)

</details>

<details>
  <summary>Granite-3.2-2B-Instruct results</summary>

![test1-prompt1-grantite-3.2-2b](/static/images/lightspeed-benchmark/test1-prompt2-granite-3.2-2b.webp)

</details>

#### Prompt 2

<details>
  <summary>GPT-4o mini results</summary>

![test1-prompt2-gpt4o-mini](/static/images/lightspeed-benchmark/test1-prompt2-gpt4o-mini.webp)

</details>

<details>
  <summary>Granite-3.2-8B-Instruct results</summary>

![test1-prompt2-grantite-3.2-8b](/static/images/lightspeed-benchmark/test1-prompt2-granite-3.2-8b.webp)

</details>

<details>
  <summary>Granite-3.2-2B-Instruct results</summary>

![test1-prompt2-grantite-3.2-2b](/static/images/lightspeed-benchmark/test1-prompt2-granite-3.2-2b.webp)

</details>

## Test 2

### Test 2 Summary problem statement

<table class="table-auto">
  <thead>
    <tr>
      <th>Problem statement</th>
      <th>Context used</th>
      <th>Prompt used</th>
      <th>Expected results</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td rowspan="2">The container image has a typo in its entrypoint command (e.g., start vs. starttt), causing the pod to start and crash immediately.</td>
      <td rowspan="2">Full Deployment YAML + full Pod YAML + full Pod log + Pod events</td>
      <td>What's wrong?</td>
      <td>The answer should identify the typo in the container’s entrypoint, recommend checking the script or command, and suggest rebuilding the image or updating the entrypoint to fix it.</td>
    </tr>
    <tr>
      <td>I am not allowed to rebuild the container image. How can I fix the issue?</td>
      <td>The answer should explain how to override the container’s command in the deployment specification to bypass the incorrect entrypoint without modifying the image.</td>
    </tr>
  </tbody>
</table>
### Test 2 Summary results
<table class="table-auto">
  <thead>
    <tr>
      <th>Prompt used</th>
      <th>Expected results</th>
      <th>GPT-4o mini</th>
      <th>Granite-3.2-8B-Instruct</th>
      <th>Granite-3.2-2B-Instruct</th>

    </tr>

  </thead>
  <tbody>
    <tr>
      <td>What's wrong?</td>
      <td>The answer should identify the typo in the container’s entrypoint, recommend checking the script or command, and suggest rebuilding the image or updating the entrypoint to fix it.</td>
      <td style={{ backgroundColor: 'moccasin' }}>While an error in the NPM script is mentioned, a typo hasn't been suggested, and the wrong script "starttt" hasn't been mentioned</td>
      <td style={{ backgroundColor: 'lightgreen' }}>The message suggest to check that the "starttt" script actually exists</td>
      <td style={{ backgroundColor: 'lightgreen' }}>The answer suggest to check the "starttt" script actually exists, and suggest to use a "start" script if it is defined</td>
    </tr>
    <tr>
      <td>I am not allowed to rebuild the container image. How can I fix the issue?</td>
      <td>The answer should explain how to override the container’s command in the deployment specification to bypass the incorrect entrypoint without modifying the image.</td>
      <td style={{ backgroundColor: 'salmon' }}>The answer provide additional legitimate way to debug things, but does not provide the right solution</td>
      <td style={{ backgroundColor: 'lightgreen' }}>The answer correctly suggest to edit the deployment with a "command" section set to the start script</td>
      <td style={{ backgroundColor: 'salmon' }}>The answer suggest to rebuild the image, or mount a volume with the script, but does not provide the right answer</td>
    </tr>
  </tbody>
</table>

### Test 2 conclusion

On this test, only granite-3.2-8b managed to correctly solve this challenge. Now, I have to be honest, during my test and exploration, depending of the prompt used, or when using multiple prompts,
it was always possible to guide gpt-4o-mini to the right answer, while it was a bit harder to get granite-3.2-2b to correctly edit the deployment (prompt 2). YMMV

### Test 2 Detailed results

#### Prompt 1

<details>
  <summary>GPT-4o mini results</summary>

![test2-prompt1-gpt4o-mini](/static/images/lightspeed-benchmark/test2-prompt1-gpt4o-mini.webp)

</details>

<details>
  <summary>Granite-3.2-8B-Instruct results</summary>

![test2-prompt1-granite-3.2-8b](/static/images/lightspeed-benchmark/test2-prompt1-granite-3.2-8b.webp)

</details>

<details>
  <summary>Granite-3.2-2B-Instruct results</summary>

![test2-prompt1-granite-3.2-2b](/static/images/lightspeed-benchmark/test2-prompt1-granite-3.2-2b.webp)

</details>

#### Prompt 2

<details>
  <summary>GPT-4o mini results</summary>

![test2-prompt2-gpt4o-mini](/static/images/lightspeed-benchmark/test2-prompt2-gpt4o-mini.webp)

</details>

<details>
  <summary>Granite-3.2-8B-Instruct results</summary>

![test2-prompt2-granite-3.2-8b](/static/images/lightspeed-benchmark/test2-prompt2-granite-3.2-8b.webp)

</details>

<details>
  <summary>Granite-3.2-2B-Instruct results</summary>

![test2-prompt2-granite-3.2-2b](/static/images/lightspeed-benchmark/test2-prompt2-granite-3.2-2b.webp)

</details>

## Test 3

### Test 3 Summary problem statement

<table class="table-auto">
  <thead>
    <tr>
      <th>Problem statement</th>
      <th>Context used</th>
      <th>Prompt used</th>
      <th>Expected results</th>
    </tr>
  </thead>
  <tbody>
        <tr>
      <td rowspan="5">The pod fails to start due to tight CPU and memory quotas in the namespace. Limits and requests must be tuned, but eventually, the deployment must be scaled down to one replica to fit within the quotas.</td>
      <td>Full Deployment YAML</td>
      <td>What's wrong?</td>
      <td>The answer should identify resource quotas as the cause and suggest setting appropriate CPU and memory requests and limits for the containers.</td>
    </tr>
      <tr>
      <td rowspan="2">Adding the namespace resource quotas</td>
      <td>What value do you suggest I should use for the deployment limits and requests?</td>
      <td>The answer should suggest resource values that fit within the quotas and ideally account for two replicas, or at least highlight the need to consider replica count when sizing limits and requests.</td>
    </tr>
    <tr>
      <td>Since I am running two replicas in the deployment, what value do you suggest I should use?</td>
      <td>The answer should clearly recommend CPU limits of 15m, memory limits of 15Mi, CPU requests of 10m, and memory requests of 7.5Mi.</td>
    </tr>
    <tr>
      <td rowspan="2">Adding one failing Pod events + full Pod YAML + full Pod logs</td>
      <td>Why is this pod not starting?</td>
      <td>The answer should explain that the pod is failing due to out-of-memory (OOM) or resource exhaustion, suggest increasing the namespace resource quotas or optimizing container resource usage, and ideally recommend scaling down to one replica.</td>
    </tr>
      <tr>
      <td>I can't optimize the application resources, and I can't change the resource quotas of the namespace. What are my other options?</td>
      <td>The answer should suggest scaling down the deployment to one replica to fit within the existing resource quotas.</td>
    </tr>
  </tbody>
</table>
### Test 3 Summary results
<table class="table-auto">
  <thead>
    <tr>
      <th>Prompt used</th>
      <th>Expected results</th>
      <th>GPT-4o mini</th>
      <th>Granite-3.2-8B-Instruct</th>
      <th>Granite-3.2-2B-Instruct</th>

    </tr>

  </thead>
  <tbody>
    <tr>
      <td>What's wrong?</td>
      <td>The answer should identify resource quotas as the cause and suggest setting appropriate CPU and memory requests and limits for the containers</td>
      <td style={{ backgroundColor: 'lightgreen' }}>Resource quotas, and lack of cpu/memory limits/requests is identified, suggesting to edit the deployment</td>
      <td style={{ backgroundColor: 'lightgreen' }}>Resource quotas, and lack of cpu/memory limits/requests is identified, suggesting to edit the deployment</td>
      <td style={{ backgroundColor: 'salmon' }}>A problem in CPU resource is identified, but not in light of resource quotas. In addition, it suggest to directly edit the pod, instead of the deployment</td>
    </tr>
    <tr>
      <td>What value do you suggest I should use for the deployment limits and requests?</td>
      <td>The answer should suggest resource values that fit within the quotas and ideally account for two replicas, or at least highlight the need to consider replica count when sizing limits and requests..</td>
      <td style={{ backgroundColor: 'moccasin' }}>It suggests to use value in light of the quota, but does not spontaneously take into account the two replicas</td>
      <td style={{ backgroundColor: 'moccasin' }}>It suggests to use value in light of the quota, but does not spontaneously take into account the two replicas</td>
      <td style={{ backgroundColor: 'salmon' }}>It suggests to use value in light of the quota, but does not spontaneously take into account the two replicas. Besides, it show a YAML example of the ResourceQuota resource, instead of the deployment</td>
    </tr>
    <tr>
      <td>Since I am running two replicas in the deployment, what value do you suggest I should use?</td>
      <td>The answer should clearly recommend CPU limits of 15m, memory limits of 15Mi, CPU requests of 10m, and memory requests of 7.5Mi..</td>
      <td style={{ backgroundColor: 'lightgreen' }}>The answer is correctly dividing the limits and requests by two, and provide a deployment example</td>
      <td style={{ backgroundColor: 'lightgreen' }}>The answer is correctly dividing the limits and requests by two, and provide a deployment example</td>
      <td style={{ backgroundColor: 'salmon' }}>The answer is still focusing on editing the resourcequota, instead of the deployment</td>
    </tr>
    <tr>
      <td>Why is this pod not starting?</td>
      <td>The answer should explain that the pod is failing due to out-of-memory (OOM) or resource exhaustion, suggest increasing the namespace resource quotas or optimizing container resource usage, and ideally recommend scaling down to one replica.</td>
      <td style={{ backgroundColor: 'lightgreen' }}>A memory issue is identified, and it is suggested to increase the limits, or optimize the application.</td>
      <td style={{ backgroundColor: 'lightgreen' }}>A memory issue is identified, and it is suggested to increase the limits, or optimize the application.</td>
      <td style={{ backgroundColor: 'lightgreen' }}>A memory issue is identified, and it is suggested to increase the limits, or optimize the application.</td>
    </tr>
    <tr>
      <td>I can't optimize the application resources, and I can't change the resource quotas of the namespace. What are my other options?</td>
      <td>The answer should suggest scaling down the deployment to one replica to fit within the existing resource quotas.</td>
      <td style={{ backgroundColor: 'salmon' }}>Scaling down is not suggested</td>
      <td style={{ backgroundColor: 'salmon' }}>Scaling down is not suggested</td>
      <td style={{ backgroundColor: 'salmon' }}>Scaling down is not suggested</td>
    </tr>
  </tbody>
</table>

### Test 3 conclusion

This test is more challenging, requires multiple prompts with the AI assistant, and the last prompt didn't led to the correct answer. However, I know that during my investigation, I got several time the correct answer,
scaling down the replica was suggested.
This only highlight the sensitivity of the answer based on slightly different prompt, or context.

### Test 3 Detailed results

#### Prompt 1

<details>
  <summary>GPT-4o mini results</summary>

![test3-prompt1-gpt4o-mini](/static/images/lightspeed-benchmark/test3-prompt1-gpt4o-mini.webp)

</details>

<details>
  <summary>Granite-3.2-8B-Instruct results</summary>

![test3-prompt1-granite-3.2-8b](/static/images/lightspeed-benchmark/test3-prompt1-granite-3.2-8b.webp)

</details>

<details>
  <summary>Granite-3.2-2B-Instruct results</summary>

![test3-prompt1-granite-3.2-2b](/static/images/lightspeed-benchmark/test3-prompt1-granite-3.2-2b.webp)

</details>

#### Prompt 2

<details>
  <summary>GPT-4o mini results</summary>

![test3-prompt2-gpt4o-mini](/static/images/lightspeed-benchmark/test3-prompt2-gpt4o-mini.webp)

</details>

<details>
  <summary>Granite-3.2-8B-Instruct results</summary>

![test3-prompt2-granite-3.2-8b](/static/images/lightspeed-benchmark/test3-prompt2-granite-3.2-8b.webp)

</details>

<details>
  <summary>Granite-3.2-2B-Instruct results</summary>

![test3-prompt2-granite-3.2-2b](/static/images/lightspeed-benchmark/test3-prompt2-granite-3.2-2b.webp)

</details>

#### Prompt 3

<details>
  <summary>GPT-4o mini results</summary>

![test3-prompt3-gpt4o-mini](/static/images/lightspeed-benchmark/test3-prompt3-gpt4o-mini.webp)

</details>

<details>
  <summary>Granite-3.2-8B-Instruct results</summary>

![test3-prompt3-granite-3.2-8b](/static/images/lightspeed-benchmark/test3-prompt3-granite-3.2-8b.webp)

</details>

<details>
  <summary>Granite-3.2-2B-Instruct results</summary>

![test3-prompt3-granite-3.2-2b](/static/images/lightspeed-benchmark/test3-prompt3-granite-3.2-2b.webp)

</details>

#### Prompt 4

<details>
  <summary>GPT-4o mini results</summary>

![test3-prompt4-gpt4o-mini](/static/images/lightspeed-benchmark/test3-prompt4-gpt4o-mini.webp)

</details>

<details>
  <summary>Granite-3.2-8B-Instruct results</summary>

![test3-prompt4-granite-3.2-8b](/static/images/lightspeed-benchmark/test3-prompt4-granite-3.2-8b.webp)

</details>

<details>
  <summary>Granite-3.2-2B-Instruct results</summary>

![test3-prompt4-granite-3.2-2b](/static/images/lightspeed-benchmark/test3-prompt4-granite-3.2-2b.webp)

</details>

#### Prompt 5

<details>
  <summary>GPT-4o mini results</summary>

![test3-prompt5-gpt4o-mini](/static/images/lightspeed-benchmark/test3-prompt5-gpt4o-mini.webp)

</details>

<details>
  <summary>Granite-3.2-8B-Instruct results</summary>

![test3-prompt5-granite-3.2-8b](/static/images/lightspeed-benchmark/test3-prompt5-granite-3.2-8b.webp)

</details>

<details>
  <summary>Granite-3.2-2B-Instruct results</summary>

![test3-prompt5-granite-3.2-2b](/static/images/lightspeed-benchmark/test3-prompt5-granite-3.2-2b.webp)

</details>

## Test 4

### Test 4 Summary problem statement

<table class="table-auto">
  <thead>
    <tr>
      <th>Problem statement</th>
      <th>Context used</th>
      <th>Prompt used</th>
      <th>Expected results</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td rowspan="1">The route is unavailable due to two misconfigurations in the service: an incorrect pod selector and an invalid port mapping.</td>
      <td>Full Deployment YAML + full Service YAML + full Route YAML</td>
      <td>What is wrong with my route or service?</td>
      <td>The answer should identify both the incorrect pod selector and the invalid service port configuration as the causes of the route being inaccessible.</td>
    </tr>
  </tbody>
</table>
### Test 4 Summary results
<table class="table-auto">
  <thead>
    <tr>
      <th>Prompt used</th>
      <th>Expected results</th>
      <th>GPT-4o mini</th>
      <th>Granite-3.2-8B-Instruct</th>
      <th>Granite-3.2-2B-Instruct</th>

    </tr>

  </thead>
  <tbody>
    <tr>
      <td>What is wrong with my route or service?</td>
      <td>The answer should identify both the incorrect pod selector and the invalid service port configuration as the causes of the route being inaccessible.</td>
      <td style={{ backgroundColor: 'lightgreen' }}>The pod selector mismatch, and the port configuration issue are both detected, and an updated yaml resource is suggested.</td>
      <td style={{ backgroundColor: 'lightgreen' }}>The pod selector mismatch, and the port configuration issue are both detected</td>
      <td style={{ backgroundColor: 'moccasin' }}>The answer is only focusing on the port mismatch, and in a desorganized way</td>
    </tr>
  </tbody>
</table>

### Test 4 conclusion

To me, and many colleague, the result of this test are very impressive, as it detected multiple problems at one. gpt-4o-mini got some extra points as it immediatly suggested
some fixes in the service yaml. It would require an extra prompt to get granite-3.2-8b to provide the YAML. granite-3.2-2b is laking a bit of reasoning power, and this was a general
observation when experimenting with test 4 and granite-3.2-2b.

### Test 4 Detailed results

#### Prompt 1

<details>
  <summary>GPT-4o mini results</summary>

![test4-prompt1-gpt4o-mini](/static/images/lightspeed-benchmark/test4-prompt1-gpt4o-mini.webp)

</details>

<details>
  <summary>Granite-3.2-8B-Instruct results</summary>

![test4-prompt1-granite-3.2-8b](/static/images/lightspeed-benchmark/test4-prompt1-granite-3.2-8b.webp)

</details>

<details>
  <summary>Granite-3.2-2B-Instruct results</summary>

![test4-prompt1-granite-3.2-2b](/static/images/lightspeed-benchmark/test4-prompt1-granite-3.2-2b.webp)

</details>

## Test 5

### Test 5 Summary problem statement

<table class="table-auto">
  <thead>
    <tr>
      <th>Problem statement</th>
      <th>Context used</th>
      <th>Prompt used</th>
      <th>Expected results</th>
    </tr>
  </thead>
  <tbody>
      <tr>
      <td rowspan="2">The pod is unschedulable because the deployment specifies a node selector label that does not exist on any node.</td>
      <td>Full Deployment YAML + full Pod YAML.</td>
      <td>Why is the pod not deploying?</td>
      <td>The answer should identify an issue with the node selector preventing pod scheduling and suggest reviewing or correcting the node selector configuration.</td>
    </tr>
    <tr>
      <td>Adding one Node YAML</td>
      <td>There is a policy that forces the use of node selector in the deployment definition. Based on the node definition attached, can you suggest some labels that are likely to be presents in all nodes, so that my pod can be scheduled everywhere?</td>
      <td>The answer should provide a corrected node selector, ideally with clear instructions and an example shown in YAML format.</td>
    </tr>
  </tbody>
</table>
### Test 5 Summary results
<table class="table-auto">
  <thead>
    <tr>
      <th>Prompt used</th>
      <th>Expected results</th>
      <th>GPT-4o mini</th>
      <th>Granite-3.2-8B-Instruct</th>
      <th>Granite-3.2-2B-Instruct</th>

    </tr>

  </thead>
  <tbody>
    <tr>
      <td>Why is the pod not deploying?</td>
      <td>The answer should identify an issue with the node selector preventing pod scheduling and suggest reviewing or correcting the node selector configuration</td>
      <td style={{ backgroundColor: 'lightgreen' }}>The node selector is mentioned as a potential issues, among other things</td>
      <td style={{ backgroundColor: 'lightgreen' }}>The node selector is mentioned as a potential issues, among other things</td>
      <td style={{ backgroundColor: 'lightgreen' }}>The node selector is mentioned as a potential issues, among other things</td>
    </tr>
        <tr>
      <td>There is a policy that forces the use of node selector in the deployment definition. Based on the node definition attached, can you suggest some labels that are likely to be presents in all nodes, so that my pod can be scheduled everywhere?</td>
      <td>The answer should provide a corrected node selector, ideally with clear instructions and an example shown in YAML format.</td>
      <td style={{ backgroundColor: 'lightgreen' }}>A list of sensible labels are suggested for the node selector</td>
      <td style={{ backgroundColor: 'lightgreen' }}>A list of sensible labels are suggested for the node selector</td>
      <td style={{ backgroundColor: 'lightgreen' }}>A list of sensible labels are suggested for the node selector</td>
    </tr>
  </tbody>
</table>

### Test 5 conclusion

All models performed correctly against this test.

### Test 5 Detailed results

#### Prompt 1

<details>
  <summary>GPT-4o mini results</summary>

![test5-prompt1-gpt4o-mini](/static/images/lightspeed-benchmark/test5-prompt1-gpt4o-mini.webp)

</details>

<details>
  <summary>Granite-3.2-8B-Instruct results</summary>

![test5-prompt1-granite-3.2-8b](/static/images/lightspeed-benchmark/test5-prompt1-granite-3.2-8b.webp)

</details>

<details>
  <summary>Granite-3.2-2B-Instruct results</summary>

![test5-prompt1-granite-3.2-2b](/static/images/lightspeed-benchmark/test5-prompt1-granite-3.2-2b.webp)

</details>

#### Prompt 2

<details>
  <summary>GPT-4o mini results</summary>

![test5-prompt2-gpt4o-mini](/static/images/lightspeed-benchmark/test5-prompt2-gpt4o-mini.webp)

</details>

<details>
  <summary>Granite-3.2-8B-Instruct results</summary>

![test5-prompt2-granite-3.2-8b](/static/images/lightspeed-benchmark/test5-prompt2-granite-3.2-8b.webp)

</details>

<details>
  <summary>Granite-3.2-2B-Instruct results</summary>

![test5-prompt2-granite-3.2-2b](/static/images/lightspeed-benchmark/test5-prompt2-granite-3.2-2b.webp)

</details>

## Test 6

### Test 6 Summary problem statement

<table class="table-auto">
  <thead>
    <tr>
      <th>Problem statement</th>
      <th>Context used</th>
      <th>Prompt used</th>
      <th>Expected results</th>
    </tr>
  </thead>
  <tbody>
        <tr>
      <td rowspan="6">A namespace is stuck in terminating state due to lingering finalizers that have not been cleared.</td>
      <td>Full Namespace YAML</td>
      <td>Why is this namespace stuck in terminating?</td>
      <td>The answer should mention that the namespace is stuck due to resources with lingering finalizers and should identify which resources are preventing termination.</td>
    </tr>
    <tr>
      <td rowspan="2">Adding the full PVC YAML</td>
      <td>Why is this PVC stuck in terminating state?</td>
      <td>The answer should identify that a finalizer is blocking the PVC deletion and suggest checking the PVC’s owner reference or related resources.</td>
    </tr>
    <tr>
      <td>Can you provide me with an openshift command line that identify any pod using this PVC?</td>
      <td>The answer should provide an oc command to identify the pod(s) using the PVC or attached to the underlying PV.</td>
    </tr>
    <tr>
      <td rowspan="2">Adding the full POD YAML</td>
      <td>Why is this pod stuck in terminating state?</td>
      <td>The answer should identify that a non-standard finalizer is blocking pod termination, and recommend removing the finalizer as part of the resolution.</td>
    </tr>
        <tr>
      <td>How can I remove the finalizer?</td>
      <td>The answer should provide an oc command to patch the resource and remove the finalizer.</td>
    </tr>
        <tr>
      <td>Downloading the secret definition, and adding it via the upload option.</td>
      <td>Why is this secret not deleted, and how can I delete it?</td>
      <td>The answer should provide an oc command to patch the Secret and remove its finalizer, allowing the resource to be deleted.</td>
    </tr>
  </tbody>
</table>
### Test 6 Summary results
<table class="table-auto">
  <thead>
    <tr>
      <th>Prompt used</th>
      <th>Expected results</th>
      <th>GPT-4o mini</th>
      <th>Granite-3.2-8B-Instruct</th>
      <th>Granite-3.2-2B-Instruct</th>

    </tr>

  </thead>
  <tbody>
    <tr>
      <td>Why is this namespace stuck in terminating?</td>
      <td>The answer should mention that the namespace is stuck due to resources with lingering finalizers and should identify which resources are preventing termination.</td>
      <td style={{ backgroundColor: 'lightgreen' }}>The answer list the dangling resources, along with their finalizer, and suggest to remove those finalizer</td>
      <td style={{ backgroundColor: 'lightgreen' }}>The answer list the dangling resources, along with their finalizer, and suggest to remove those finalizer</td>
      <td style={{ backgroundColor: 'lightgreen' }}>The answer list the dangling resources, along with their finalizer, and suggest to remove those finalizer</td>
    </tr>
    <tr>
      <td>Why is this PVC stuck in terminating state?</td>
      <td>The answer should identify that a finalizer is blocking the PVC deletion and suggest checking the PVC’s owner reference or related resources.</td>
      <td style={{ backgroundColor: 'lightgreen' }}>The answer explains why there is still a finalizer on the PVC, suggest to check the status of the associated PV, and provides instruction to remove the finalizer if it's safe to do so</td>
      <td style={{ backgroundColor: 'moccasin' }}>The answer explains why there is still a finalizer on the PVC, and provides instruction on how to remove it. However, it does not provide any hint that one should check if the PVC is still being used.</td>
      <td style={{ backgroundColor: 'lightgreen' }}>The answer explains why there is still a finalizer on the PVC, suggest to delete the finalizer, and also suggest to check for resources that are still using the PVC</td>
    </tr>
        <tr>
      <td>Can you provide me with an openshift command line that identify any pod using this PVC?</td>
      <td>The answer should provide an oc command to identify the pod(s) using the PVC or attached to the underlying PV.</td>
      <td style={{ backgroundColor: 'lightgreen' }}>The oc command provided works as intended</td>
      <td style={{ backgroundColor: 'salmon' }}>The oc command did not work, potentially due to this known [issue](https://access.redhat.com/solutions/7004229)</td>
      <td style={{ backgroundColor: 'salmon' }}>The oc command did not work</td>
    </tr>
        <tr>
      <td>Why is this pod stuck in terminating state?</td>
      <td>The answer should identify that a non-standard finalizer is blocking pod termination, and recommend removing the finalizer as part of the resolution.</td>
  
      <td style={{ backgroundColor: 'lightgreen' }}>The answer correctly identify the non-standard finalizer, and suggest to remove it with an oc edit command</td>
      <td style={{ backgroundColor: 'lightgreen' }}>The answer correctly identify the non-standard finalizer, and suggest to remove it with an oc patch command, which is more convenient</td>
      <td style={{ backgroundColor: 'salmon' }}>The issue with the finalizer is not identified</td>
    </tr>

        <tr>
      <td>How can I remove the finalizer?</td>
      <td>The answer should provide an oc command to patch the resource and remove the finalizer.</td>
      <td style={{ backgroundColor: 'lightgreen' }}>An oc edit command with instructions is provided</td>
      <td style={{ backgroundColor: 'lightgreen' }}>A more convenient oc patch command is provided</td>
      <td style={{ backgroundColor: 'moccasin' }}>Generic instructions with oc patch is provided for pvc or pod. However, I would expect the instruction to focus on pod, and use the specific pod name, instead of a generic name</td>
    </tr>
        <tr>
      <td>Why is this secret not deleted, and how can I delete it?</td>
      <td>The answer should provide an oc command to patch the Secret and remove its finalizer, allowing the resource to be deleted.</td>
      <td style={{ backgroundColor: 'lightgreen' }}>oc edit command instructions for the secret is provided</td>
      <td style={{ backgroundColor: 'lightgreen' }}>oc patch command instructions for the secret is provided</td>
      <td style={{ backgroundColor: 'lightgreen' }}>oc patch command instructions for the secret is provided</td>
    </tr>

  </tbody>
</table>

### Test 6 conclusion

gpt4o-mini succesfully answered all prompts. grantie-3.2-8b had an issue with the oc commands, probably due to a known issue. However, the oc command suggested was more complex that needed. As usual, prompting differently
yield different results, YMMV. granite-3.2-2b just doesn't have the reasoning capabilities for these challenge, and it would take several prompt attempt to get to the right answer.

### Test 6 Detailed results

#### Prompt 1

<details>
  <summary>GPT-4o mini results</summary>

![test6-prompt1-gpt4o-mini](/static/images/lightspeed-benchmark/test6-prompt1-gpt4o-mini.webp)

</details>

<details>
  <summary>Granite-3.2-8B-Instruct results</summary>

![test6-prompt1-granite-3.2-8b](/static/images/lightspeed-benchmark/test6-prompt1-granite-3.2-8b.webp)

</details>

<details>
  <summary>Granite-3.2-2B-Instruct results</summary>

![test6-prompt1-granite-3.2-2b](/static/images/lightspeed-benchmark/test6-prompt1-granite-3.2-2b.webp)

</details>

#### Prompt 2

<details>
  <summary>GPT-4o mini results</summary>

![test6-prompt2-gpt4o-mini](/static/images/lightspeed-benchmark/test6-prompt2-gpt4o-mini.webp)

</details>

<details>
  <summary>Granite-3.2-8B-Instruct results</summary>

![test6-prompt2-granite-3.2-8b](/static/images/lightspeed-benchmark/test6-prompt2-granite-3.2-8b.webp)

</details>

<details>
  <summary>Granite-3.2-2B-Instruct results</summary>

![test6-prompt2-granite-3.2-2b](/static/images/lightspeed-benchmark/test6-prompt2-granite-3.2-2b.webp)

</details>

#### Prompt 3

<details>
  <summary>GPT-4o mini results</summary>

![test6-prompt3-gpt4o-mini](/static/images/lightspeed-benchmark/test6-prompt3-gpt4o-mini.webp)

</details>

<details>
  <summary>Granite-3.2-8B-Instruct results</summary>

![test6-prompt3-granite-3.2-8b](/static/images/lightspeed-benchmark/test6-prompt3-granite-3.2-8b.webp)

</details>

<details>
  <summary>Granite-3.2-2B-Instruct results</summary>

![test6-prompt3-granite-3.2-2b](/static/images/lightspeed-benchmark/test6-prompt3-granite-3.2-2b.webp)

</details>

#### Prompt 4

<details>
  <summary>GPT-4o mini results</summary>

![test6-prompt4-gpt4o-mini](/static/images/lightspeed-benchmark/test6-prompt4-gpt4o-mini.webp)

</details>

<details>
  <summary>Granite-3.2-8B-Instruct results</summary>

![test6-prompt4-granite-3.2-8b](/static/images/lightspeed-benchmark/test6-prompt4-granite-3.2-8b.webp)

</details>

<details>
  <summary>Granite-3.2-2B-Instruct results</summary>

![test6-prompt4-granite-3.2-2b](/static/images/lightspeed-benchmark/test6-prompt4-granite-3.2-2b.webp)

</details>

#### Prompt 5

<details>
  <summary>GPT-4o mini results</summary>

![test6-prompt5-gpt4o-mini](/static/images/lightspeed-benchmark/test6-prompt5-gpt4o-mini.webp)

</details>

<details>
  <summary>Granite-3.2-8B-Instruct results</summary>

![test6-prompt5-granite-3.2-8b](/static/images/lightspeed-benchmark/test6-prompt5-granite-3.2-8b.webp)

</details>

<details>
  <summary>Granite-3.2-2B-Instruct results</summary>

![test6-prompt5-granite-3.2-2b](/static/images/lightspeed-benchmark/test6-prompt5-granite-3.2-2b.webp)

</details>

#### Prompt 6

<details>
  <summary>GPT-4o mini results</summary>

![test6-prompt6-gpt4o-mini](/static/images/lightspeed-benchmark/test6-prompt6-gpt4o-mini.webp)

</details>

<details>
  <summary>Granite-3.2-8B-Instruct results</summary>

![test6-prompt6-granite-3.2-8b](/static/images/lightspeed-benchmark/test6-prompt6-granite-3.2-8b.webp)

</details>

<details>
  <summary>Granite-3.2-2B-Instruct results</summary>

![test6-prompt6-granite-3.2-2b](/static/images/lightspeed-benchmark/test6-prompt6-granite-3.2-2b.webp)

</details>

# Conclusion

This study highlight a few things:

1. OpenShift AI assistant is already incredibly useful. It will only get better as it goes into General Availability with additional features. Bear in mind that the LLM is not trained against OpenShift,
   but it leverage RAG technique against the embeded OpenShift documentation to be OpenShift aware.
2. A self hosted 8 billions LLM with good reasoning capabilities is plenty enough for OpenShift users that can't send their queries to a cloud hosted LLM provider (compliance, disconnected)
   8B parameters easily fit in high end general consumer GPU / low end enterprise GPU. Context memory, and of course concurrency must also be taken into account when sizing the memory requirements.
3. Too small LLM (2Billions parameters) are not yet fit for purpose. The AI industry is moving so fast that this might change in the near future.
4. Prompting is not an exact science in general, it remains the case with LightSpeed. Rephrasing questions differently does yield different results. Having say that, on granite-3.2-8b, or gpt-4o-mini, it has always
   been possible to drive LightSpeed toward a helpful answer.
